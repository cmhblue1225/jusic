"""
ğŸ”¥ Phase 2.2: ë„¤ì´ë²„ ì¦ê¶Œ í† ë¡ ë°© í¬ë¡¤ë§
íˆ¬ìì ì‹¬ë¦¬ ë° ì»¤ë®¤ë‹ˆí‹° ì˜ê²¬ ìˆ˜ì§‘
"""
import os
import asyncio
import aiohttp
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import re

load_dotenv()


class NaverDiscussionCrawler:
    """ë„¤ì´ë²„ ì¦ê¶Œ í† ë¡ ë°© í¬ë¡¤ëŸ¬"""

    def __init__(self):
        self.base_url = "https://finance.naver.com"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Referer": "https://finance.naver.com/"
        }

    async def crawl_discussions(
        self,
        symbol: str,
        days: int = 7,
        limit: int = 50
    ) -> List[Dict[str, Any]]:
        """
        íŠ¹ì • ì¢…ëª©ì˜ í† ë¡ ë°© ê²Œì‹œê¸€ í¬ë¡¤ë§

        Args:
            symbol: ì¢…ëª© ì½”ë“œ (ì˜ˆ: '005930')
            days: ìˆ˜ì§‘ ê¸°ê°„ (ì¼)
            limit: ìµœëŒ€ ê²Œì‹œê¸€ ìˆ˜

        Returns:
            List[Dict]: í† ë¡ ë°© ê²Œì‹œê¸€ ëª©ë¡
        """
        discussions = []
        cutoff_date = datetime.now() - timedelta(days=days)

        try:
            async with aiohttp.ClientSession(headers=self.headers) as session:
                # í˜ì´ì§€ë³„ë¡œ í¬ë¡¤ë§ (í•œ í˜ì´ì§€ë‹¹ 20ê°œ ê²Œì‹œê¸€)
                page = 1
                max_pages = (limit // 20) + 1

                while len(discussions) < limit and page <= max_pages:
                    url = f"{self.base_url}/item/board.naver?code={symbol}&page={page}"

                    async with session.get(url, timeout=10) as response:
                        if response.status != 200:
                            print(f"âš ï¸ HTTP {response.status}: {url}")
                            break

                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')

                        # ê²Œì‹œê¸€ í…Œì´ë¸” ì°¾ê¸°
                        table = soup.find('table', class_='type2')
                        if not table:
                            print(f"âš ï¸ í† ë¡ ë°© í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (page {page})")
                            break

                        rows = table.find('tbody').find_all('tr')

                        for row in rows:
                            try:
                                # ê³µì§€ì‚¬í•­ ì œì™¸
                                if 'notice' in row.get('class', []):
                                    continue

                                # ì œëª© ë° ë§í¬
                                title_elem = row.find('td', class_='title')
                                if not title_elem:
                                    continue

                                link_elem = title_elem.find('a')
                                if not link_elem:
                                    continue

                                title = link_elem.get_text(strip=True)
                                link = self.base_url + link_elem.get('href', '')

                                # ì‘ì„±ì
                                writer_elem = row.find('td', class_='p11')
                                writer = writer_elem.get_text(strip=True) if writer_elem else "Unknown"

                                # ì¡°íšŒìˆ˜
                                view_elem = row.find('td', class_='p10')
                                view_count = 0
                                if view_elem:
                                    view_text = view_elem.get_text(strip=True)
                                    view_count = int(view_text) if view_text.isdigit() else 0

                                # ì¢‹ì•„ìš”/ê³µê°ìˆ˜
                                like_elem = row.find('td', class_='p9')
                                like_count = 0
                                if like_elem:
                                    like_text = like_elem.get_text(strip=True)
                                    like_count = int(like_text) if like_text.isdigit() else 0

                                # ë‚ ì§œ
                                date_elem = row.find('td', class_='p11', attrs={'align': 'center'})
                                if not date_elem:
                                    date_elem = row.find_all('td', class_='p11')[-1] if row.find_all('td', class_='p11') else None

                                date_str = date_elem.get_text(strip=True) if date_elem else ""
                                published_at = self._parse_date(date_str)

                                # ê¸°ê°„ í•„í„°ë§
                                if published_at and published_at < cutoff_date:
                                    print(f"  â„¹ï¸ ê¸°ê°„ ì´ˆê³¼ ê²Œì‹œê¸€ ë°œê²¬ (page {page}), í¬ë¡¤ë§ ì¤‘ë‹¨")
                                    break

                                discussion = {
                                    "title": title,
                                    "link": link,
                                    "writer": writer,
                                    "view_count": view_count,
                                    "like_count": like_count,
                                    "published_at": published_at.isoformat() if published_at else None,
                                    "source": "naver_discussion",
                                    "symbol": symbol
                                }

                                discussions.append(discussion)

                                if len(discussions) >= limit:
                                    break

                            except Exception as e:
                                print(f"  âš ï¸ ê²Œì‹œê¸€ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                                continue

                        # ê¸°ê°„ ì´ˆê³¼ ì‹œ ì¤‘ë‹¨
                        if len(discussions) > 0 and discussions[-1].get("published_at"):
                            last_date = datetime.fromisoformat(discussions[-1]["published_at"])
                            if last_date < cutoff_date:
                                break

                    page += 1
                    await asyncio.sleep(0.5)  # Rate limiting

            print(f"âœ… ë„¤ì´ë²„ í† ë¡ ë°© í¬ë¡¤ë§ ì™„ë£Œ: {len(discussions)}ê°œ")
            return discussions[:limit]

        except Exception as e:
            print(f"âŒ í† ë¡ ë°© í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
            return []

    async def get_discussion_content(
        self,
        discussion_url: str
    ) -> Optional[str]:
        """
        ê²Œì‹œê¸€ ë³¸ë¬¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°

        Args:
            discussion_url: ê²Œì‹œê¸€ URL

        Returns:
            str: ê²Œì‹œê¸€ ë³¸ë¬¸ (ì—†ìœ¼ë©´ None)
        """
        try:
            async with aiohttp.ClientSession(headers=self.headers) as session:
                async with session.get(discussion_url, timeout=10) as response:
                    if response.status != 200:
                        return None

                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')

                    # ë³¸ë¬¸ ì°¾ê¸°
                    content_elem = soup.find('div', class_='board_txt')
                    if not content_elem:
                        content_elem = soup.find('div', class_='view_info')

                    if content_elem:
                        # HTML íƒœê·¸ ì œê±° ë° ì •ë¦¬
                        content = content_elem.get_text(separator='\n', strip=True)
                        content = re.sub(r'\n{3,}', '\n\n', content)  # ê³¼ë„í•œ ì¤„ë°”ê¿ˆ ì œê±°
                        return content

                    return None

        except Exception as e:
            print(f"  âš ï¸ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}")
            return None

    def _parse_date(self, date_str: str) -> Optional[datetime]:
        """
        ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹±

        Args:
            date_str: ë‚ ì§œ ë¬¸ìì—´ (ì˜ˆ: '2025.01.18', '01.18', '15:30')

        Returns:
            datetime: íŒŒì‹±ëœ ë‚ ì§œ
        """
        try:
            now = datetime.now()

            # ì˜¤ëŠ˜ ë‚ ì§œ (ì‹œê°„ë§Œ í‘œì‹œ: '15:30')
            if ':' in date_str and '.' not in date_str:
                time_parts = date_str.split(':')
                hour = int(time_parts[0])
                minute = int(time_parts[1])
                return now.replace(hour=hour, minute=minute, second=0, microsecond=0)

            # ì›”.ì¼ í˜•ì‹ ('01.18')
            elif date_str.count('.') == 1:
                month, day = map(int, date_str.split('.'))
                # ì˜¬í•´ë¡œ ê°€ì •
                year = now.year
                # ë§Œì•½ ë¯¸ë˜ ë‚ ì§œë©´ ì‘ë…„ìœ¼ë¡œ ì¡°ì •
                dt = datetime(year, month, day)
                if dt > now:
                    dt = datetime(year - 1, month, day)
                return dt

            # ë…„.ì›”.ì¼ í˜•ì‹ ('2025.01.18')
            elif date_str.count('.') == 2:
                year, month, day = map(int, date_str.split('.'))
                return datetime(year, month, day)

            else:
                print(f"  âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ë‚ ì§œ í˜•ì‹: {date_str}")
                return None

        except Exception as e:
            print(f"  âš ï¸ ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜ ({date_str}): {str(e)}")
            return None

    async def analyze_sentiment_from_discussions(
        self,
        discussions: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        í† ë¡ ë°© ê²Œì‹œê¸€ë¡œë¶€í„° íˆ¬ì ì‹¬ë¦¬ ë¶„ì„

        Args:
            discussions: í† ë¡ ë°© ê²Œì‹œê¸€ ëª©ë¡

        Returns:
            Dict: ì‹¬ë¦¬ ë¶„ì„ ê²°ê³¼
                - sentiment_score: ê°ì„± ì ìˆ˜ (-1 ~ 1)
                - bullish_count: ê¸ì • ê²Œì‹œê¸€ ìˆ˜
                - bearish_count: ë¶€ì • ê²Œì‹œê¸€ ìˆ˜
                - neutral_count: ì¤‘ë¦½ ê²Œì‹œê¸€ ìˆ˜
                - total_count: ì „ì²´ ê²Œì‹œê¸€ ìˆ˜
                - avg_view_count: í‰ê·  ì¡°íšŒìˆ˜
                - avg_like_count: í‰ê·  ê³µê°ìˆ˜
                - hot_topics: ì£¼ìš” í‚¤ì›Œë“œ
        """
        if not discussions:
            return {
                "sentiment_score": 0,
                "bullish_count": 0,
                "bearish_count": 0,
                "neutral_count": 0,
                "total_count": 0,
                "avg_view_count": 0,
                "avg_like_count": 0,
                "hot_topics": []
            }

        # ê¸ì •/ë¶€ì • í‚¤ì›Œë“œ
        bullish_keywords = [
            "ê¸‰ë“±", "ìƒìŠ¹", "ë§¤ìˆ˜", "ë¶ˆíƒ€", "ì¶”ê°€ë§¤ìˆ˜", "ì˜¤ë¥¸ë‹¤", "ì˜¬ë¼", "ê°•ì„¸",
            "í˜¸ì¬", "ê¸°ëŒ€", "ì¢‹ë‹¤", "ìµœê³ ", "ëŒ€ë°•", "ìŒë”°ë´‰", "ê°‘ë‹ˆë‹¤", "ê°€ì¦ˆì•„"
        ]
        bearish_keywords = [
            "ê¸‰ë½", "í•˜ë½", "ë§¤ë„", "ì†ì ˆ", "ë–¨ì–´", "ë‚´ë ¤", "ì•½ì„¸", "ì•…ì¬",
            "ë§í–ˆ", "ë¬¼ë ¸", "ìµœì•…", "í­ë½", "ë¹ ì§„ë‹¤", "ì¡°ì‹¬", "ìœ„í—˜"
        ]

        bullish_count = 0
        bearish_count = 0
        neutral_count = 0
        total_views = 0
        total_likes = 0

        for discussion in discussions:
            title = discussion.get("title", "").lower()

            # ì œëª©ì—ì„œ ê°ì„± íŒë‹¨
            is_bullish = any(keyword in title for keyword in bullish_keywords)
            is_bearish = any(keyword in title for keyword in bearish_keywords)

            if is_bullish and not is_bearish:
                bullish_count += 1
            elif is_bearish and not is_bullish:
                bearish_count += 1
            else:
                neutral_count += 1

            total_views += discussion.get("view_count", 0)
            total_likes += discussion.get("like_count", 0)

        total_count = len(discussions)
        avg_view_count = total_views / total_count if total_count > 0 else 0
        avg_like_count = total_likes / total_count if total_count > 0 else 0

        # ê°ì„± ì ìˆ˜ ê³„ì‚° (-1 ~ 1)
        if total_count > 0:
            sentiment_score = (bullish_count - bearish_count) / total_count
        else:
            sentiment_score = 0

        # ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë¹ˆë„ ë¶„ì„)
        all_titles = " ".join([d.get("title", "") for d in discussions])
        hot_topics = self._extract_hot_topics(all_titles)

        return {
            "sentiment_score": round(sentiment_score, 3),
            "bullish_count": bullish_count,
            "bearish_count": bearish_count,
            "neutral_count": neutral_count,
            "total_count": total_count,
            "avg_view_count": round(avg_view_count, 1),
            "avg_like_count": round(avg_like_count, 1),
            "hot_topics": hot_topics[:10],  # ìƒìœ„ 10ê°œ
            "community_sentiment": "bullish" if sentiment_score > 0.2 else "bearish" if sentiment_score < -0.2 else "neutral"
        }

    def _extract_hot_topics(self, text: str) -> List[str]:
        """ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë¹ˆë„ ë¶„ì„)"""
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = [
            "ëŠ”", "ì€", "ì´", "ê°€", "ì„", "ë¥¼", "ì˜", "ì—", "ì™€", "ê³¼", "ë„",
            "ìœ¼ë¡œ", "ì—ì„œ", "í•œ", "í•˜ë‹¤", "ìˆë‹¤", "ë˜ë‹¤", "ì—†ë‹¤", "ì•„ë‹ˆë‹¤",
            "ê·¸", "ì €", "ì´", "ê·¸ê±°", "ì €ê±°", "ë­", "ì¢€", "ë”", "ì˜", "ì•ˆ"
        ]

        # ë‹¨ì–´ ë¶„ë¦¬ (ê°„ë‹¨í•œ ê³µë°± ê¸°ë°˜)
        words = re.findall(r'\b\w{2,}\b', text)
        word_freq = {}

        for word in words:
            if word not in stopwords and len(word) >= 2:
                word_freq[word] = word_freq.get(word, 0) + 1

        # ë¹ˆë„ìˆœ ì •ë ¬
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        return [word for word, freq in sorted_words if freq >= 2]  # ìµœì†Œ 2íšŒ ì´ìƒ ë“±ì¥


async def main():
    """í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    crawler = NaverDiscussionCrawler()

    # ì‚¼ì„±ì „ì í† ë¡ ë°© í¬ë¡¤ë§
    symbol = "005930"
    print(f"ğŸ” ë„¤ì´ë²„ í† ë¡ ë°© í¬ë¡¤ë§ ì‹œì‘: {symbol}")

    discussions = await crawler.crawl_discussions(symbol, days=7, limit=30)
    print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ: {len(discussions)}ê°œ ê²Œì‹œê¸€")

    if discussions:
        print("\nğŸ“Š ìµœê·¼ ê²Œì‹œê¸€ 5ê°œ:")
        for i, disc in enumerate(discussions[:5], 1):
            print(f"{i}. {disc['title']}")
            print(f"   ì‘ì„±ì: {disc['writer']} | ì¡°íšŒ: {disc['view_count']} | ê³µê°: {disc['like_count']}")
            print(f"   ë‚ ì§œ: {disc['published_at']}")
            print()

        # íˆ¬ì ì‹¬ë¦¬ ë¶„ì„
        sentiment = await crawler.analyze_sentiment_from_discussions(discussions)
        print("\nğŸ’­ ì»¤ë®¤ë‹ˆí‹° íˆ¬ì ì‹¬ë¦¬:")
        print(f"  - ê°ì„± ì ìˆ˜: {sentiment['sentiment_score']:.3f}")
        print(f"  - ê¸ì • ê²Œì‹œê¸€: {sentiment['bullish_count']}ê°œ")
        print(f"  - ë¶€ì • ê²Œì‹œê¸€: {sentiment['bearish_count']}ê°œ")
        print(f"  - ì¤‘ë¦½ ê²Œì‹œê¸€: {sentiment['neutral_count']}ê°œ")
        print(f"  - í‰ê·  ì¡°íšŒìˆ˜: {sentiment['avg_view_count']:.1f}")
        print(f"  - í‰ê·  ê³µê°ìˆ˜: {sentiment['avg_like_count']:.1f}")
        print(f"  - ì»¤ë®¤ë‹ˆí‹° ì‹¬ë¦¬: {sentiment['community_sentiment'].upper()}")
        print(f"  - ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(sentiment['hot_topics'][:10])}")


if __name__ == "__main__":
    asyncio.run(main())
