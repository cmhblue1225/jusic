"""
ì‹¤ì‹œê°„ ë‰´ìŠ¤ í¬ë¡¤ë§ ë° AI ë¶„ì„ ëª¨ë“ˆ (Report Serviceìš©)

í•˜ì´ë¸Œë¦¬ë“œ ë‰´ìŠ¤ fetching ì „ëµ:
1. DBì—ì„œ ìµœì‹  ë‰´ìŠ¤ ì¡°íšŒ
2. ìµœì‹  ë‰´ìŠ¤ê°€ 12ì‹œê°„ ì´ìƒ ì˜¤ë˜ë˜ì—ˆìœ¼ë©´ ì‹¤ì‹œê°„ í¬ë¡¤ë§ íŠ¸ë¦¬ê±°
3. AI ë¶„ì„ í›„ DBì— ì €ì¥
4. DB ë‰´ìŠ¤ + ì‹ ê·œ ë‰´ìŠ¤ ë³‘í•©í•˜ì—¬ ë°˜í™˜
"""
import os
import httpx
import feedparser
import re
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional, Tuple, Any
from urllib.parse import quote_plus
from supabase import create_client, Client

# í™˜ê²½ ë³€ìˆ˜
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_SERVICE_KEY = os.getenv("SUPABASE_SERVICE_KEY")
AI_SERVICE_URL = os.getenv("AI_SERVICE_URL", "http://localhost:8002")
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

# Supabase í´ë¼ì´ì–¸íŠ¸ (Lazy Initialization)
_supabase_client = None

def get_supabase_client() -> Client:
    """Supabase í´ë¼ì´ì–¸íŠ¸ Lazy ì´ˆê¸°í™”"""
    global _supabase_client
    if _supabase_client is None:
        if not SUPABASE_URL or not SUPABASE_SERVICE_KEY:
            raise ValueError("SUPABASE_URL ë° SUPABASE_SERVICE_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        _supabase_client = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)
    return _supabase_client


class NaverNewsAPI:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self):
        self.client_id = NAVER_CLIENT_ID
        self.client_secret = NAVER_CLIENT_SECRET
        self.base_url = "https://openapi.naver.com/v1/search/news.json"

        if not self.client_id or not self.client_secret:
            print("âš ï¸ ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    def clean_html_tags(self, text: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        clean_text = re.sub(r'<[^>]+>', '', text)
        clean_text = clean_text.replace('&quot;', '"')
        clean_text = clean_text.replace('&amp;', '&')
        clean_text = clean_text.replace('&lt;', '<')
        clean_text = clean_text.replace('&gt;', '>')
        return clean_text

    async def search_stock_news(
        self,
        stock_name: str,
        max_results: int = 10
    ) -> List[Dict]:
        """íŠ¹ì • ì¢…ëª© ê´€ë ¨ ë‰´ìŠ¤ ê²€ìƒ‰"""
        if not self.client_id or not self.client_secret:
            return []

        try:
            headers = {
                "X-Naver-Client-Id": self.client_id,
                "X-Naver-Client-Secret": self.client_secret,
            }

            params = {
                "query": stock_name,
                "display": max_results,
                "start": 1,
                "sort": "date",
            }

            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(
                    self.base_url,
                    headers=headers,
                    params=params
                )

                if response.status_code == 200:
                    data = response.json()
                    items = data.get("items", [])

                    # íŒŒì‹±
                    parsed_news = []
                    for item in items:
                        title = self.clean_html_tags(item.get("title", ""))
                        description = self.clean_html_tags(item.get("description", ""))

                        # ë‚ ì§œ íŒŒì‹±
                        pub_date_str = item.get("pubDate", "")
                        try:
                            pub_date = datetime.strptime(pub_date_str, "%a, %d %b %Y %H:%M:%S %z")
                        except:
                            pub_date = datetime.now(timezone.utc)

                        parsed_news.append({
                            "title": title,
                            "content": description,
                            "url": item.get("originallink") or item.get("link"),
                            "published_at": pub_date.isoformat(),
                            "source": "Naver ë‰´ìŠ¤ ê²€ìƒ‰",
                        })

                    return parsed_news
                else:
                    print(f"âš ï¸ ë„¤ì´ë²„ API ì˜¤ë¥˜: {response.status_code}")
                    return []

        except Exception as e:
            print(f"âŒ ë„¤ì´ë²„ API í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}")
            return []


class GoogleNewsRSS:
    """Google News RSS Feed í¬ë¡¤ëŸ¬"""

    def __init__(self):
        self.base_url = "https://news.google.com/rss/search"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
        }

    async def search_stock_news(
        self,
        stock_name: str,
        max_results: int = 10
    ) -> List[Dict[str, Any]]:
        """íŠ¹ì • ì¢…ëª©ëª…ìœ¼ë¡œ Google News ê²€ìƒ‰"""
        try:
            query = f"{stock_name} ì£¼ì‹"
            encoded_query = quote_plus(query)

            rss_url = (
                f"{self.base_url}?q={encoded_query}"
                f"&hl=ko"
                f"&gl=KR"
                f"&ceid=KR:ko"
            )

            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(rss_url, headers=self.headers)
                response.raise_for_status()

            feed = feedparser.parse(response.text)

            if not feed.entries:
                return []

            news_list = []
            for entry in feed.entries[:max_results]:
                try:
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        published_dt = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                        published_iso = published_dt.isoformat()
                    else:
                        published_iso = datetime.now(timezone.utc).isoformat()

                    content = ""
                    if hasattr(entry, 'summary'):
                        content = entry.summary
                    elif hasattr(entry, 'description'):
                        content = entry.description

                    if content:
                        content = re.sub(r'<[^>]+>', '', content).strip()

                    news_list.append({
                        "title": entry.title,
                        "url": entry.link,
                        "published_at": published_iso,
                        "source": "Google News",
                        "content": content[:500] if content else ""
                    })

                except Exception as e:
                    print(f"âš ï¸ [Google News RSS] í•­ëª© íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                    continue

            return news_list

        except Exception as e:
            print(f"âŒ [Google News RSS] ì˜¤ë¥˜: {str(e)}")
            return []


async def check_news_freshness(
    symbol: str,
    threshold_hours: int = 12
) -> Tuple[bool, Optional[datetime]]:
    """
    ë‰´ìŠ¤ ì‹ ì„ ë„ í™•ì¸

    Args:
        symbol: ì¢…ëª© ì½”ë“œ (ì˜ˆ: "005930")
        threshold_hours: ì‹ ì„ ë„ ì„ê³„ê°’ (ì‹œê°„ ë‹¨ìœ„)

    Returns:
        (is_fresh, latest_news_timestamp)
        - is_fresh: True if ìµœì‹  ë‰´ìŠ¤ < threshold_hours
        - latest_news_timestamp: ìµœì‹  ë‰´ìŠ¤ ë°œí–‰ ì‹œê°„
    """
    try:
        result = get_supabase_client().table("news") \
            .select("published_at") \
            .contains("related_symbols", [symbol]) \
            .order("published_at", desc=True) \
            .limit(1) \
            .execute()

        if not result.data:
            print(f"âš ï¸ ë‰´ìŠ¤ ì‹ ì„ ë„ ì²´í¬: {symbol} - DBì— ë‰´ìŠ¤ ì—†ìŒ (ì‹¤ì‹œê°„ í¬ë¡¤ë§ í•„ìš”)")
            return False, None

        latest_news = result.data[0]
        published_at_str = latest_news["published_at"]
        published_at = datetime.fromisoformat(published_at_str.replace('Z', '+00:00'))

        now = datetime.now(timezone.utc)
        age_hours = (now - published_at).total_seconds() / 3600

        is_fresh = age_hours < threshold_hours

        if is_fresh:
            print(f"âœ… ë‰´ìŠ¤ ì‹ ì„ ë„ ì²´í¬: {symbol} - ìµœì‹  ë‰´ìŠ¤ {age_hours:.1f}ì‹œê°„ ì „ (DB ì‚¬ìš©)")
        else:
            print(f"âš ï¸ ë‰´ìŠ¤ ì‹ ì„ ë„ ì²´í¬: {symbol} - ìµœì‹  ë‰´ìŠ¤ {age_hours:.1f}ì‹œê°„ ì „ (ì‹¤ì‹œê°„ í¬ë¡¤ë§ íŠ¸ë¦¬ê±°)")

        return is_fresh, published_at

    except Exception as e:
        print(f"âŒ ë‰´ìŠ¤ ì‹ ì„ ë„ ì²´í¬ ì˜¤ë¥˜: {str(e)}")
        return False, None


async def fetch_realtime_news(
    symbol: str,
    stock_name: str,
    max_results: int = 10
) -> List[Dict]:
    """
    ì‹¤ì‹œê°„ ë‰´ìŠ¤ í¬ë¡¤ë§ (Naver + Google RSS)

    Args:
        symbol: ì¢…ëª© ì½”ë“œ
        stock_name: ì¢…ëª©ëª…
        max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜

    Returns:
        ì›ì‹œ ë‰´ìŠ¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (AI ë¶„ì„ ì „)
    """
    print(f"ğŸ”„ ì‹¤ì‹œê°„ í¬ë¡¤ë§ ì‹œì‘: {stock_name} ({symbol})")

    all_news = []
    seen_urls = set()

    # 1. ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§
    try:
        naver_api = NaverNewsAPI()
        naver_news = await naver_api.search_stock_news(stock_name, max_results=max_results)

        for news in naver_news:
            url = news["url"]
            if url not in seen_urls:
                seen_urls.add(url)
                all_news.append(news)

        print(f"âœ… ë„¤ì´ë²„ ë‰´ìŠ¤: {len(naver_news)}ê°œ ìˆ˜ì§‘")
    except Exception as e:
        print(f"âš ï¸ ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")

    # 2. Google News RSS í¬ë¡¤ë§
    try:
        google_rss = GoogleNewsRSS()
        google_news = await google_rss.search_stock_news(stock_name, max_results=max_results)

        for news in google_news:
            url = news["url"]
            if url not in seen_urls:
                seen_urls.add(url)
                all_news.append(news)

        print(f"âœ… Google News: {len(google_news)}ê°œ ìˆ˜ì§‘")
    except Exception as e:
        print(f"âš ï¸ Google News í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")

    print(f"ğŸ“Š ì‹¤ì‹œê°„ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_news)}ê°œ (ì¤‘ë³µ ì œê±° í›„)")
    return all_news


async def analyze_and_save_news(
    news_items: List[Dict],
    symbol: str
) -> List[Dict]:
    """
    AI ë¶„ì„ í›„ Supabaseì— ì €ì¥

    Args:
        news_items: ì›ì‹œ ë‰´ìŠ¤ ë°ì´í„°
        symbol: ì¢…ëª© ì½”ë“œ

    Returns:
        ë¶„ì„ëœ ë‰´ìŠ¤ ë°ì´í„° (DB ì €ì¥ ì™„ë£Œ)
    """
    if not news_items:
        return []

    print(f"ğŸ¤– AI ë¶„ì„ ì‹œì‘: {len(news_items)}ê°œ ë‰´ìŠ¤")

    analyzed_news = []

    for news in news_items:
        try:
            # AI ë¶„ì„ ìš”ì²­
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    f"{AI_SERVICE_URL}/analyze",
                    json={
                        "title": news["title"],
                        "content": news["content"],
                        "symbols": [symbol],
                        "url": news["url"]
                    }
                )

                if response.status_code == 200:
                    ai_result = response.json()

                    # DB ì €ì¥ìš© ë°ì´í„° ì¤€ë¹„
                    news_data = {
                        "title": news["title"],
                        "url": news["url"],
                        "source": news["source"],
                        "published_at": news["published_at"],
                        "summary": ai_result.get("summary"),
                        "sentiment_score": ai_result.get("sentiment_score"),
                        "impact_score": ai_result.get("impact_score"),
                        "recommended_action": ai_result.get("recommended_action"),
                        "related_symbols": [symbol],
                    }

                    # Supabaseì— ì €ì¥
                    try:
                        get_supabase_client().table("news").insert(news_data).execute()
                        analyzed_news.append(news_data)
                        print(f"âœ… AI ë¶„ì„ + ì €ì¥ ì„±ê³µ: {news['title'][:50]}...")
                    except Exception as e:
                        print(f"âš ï¸ DB ì €ì¥ ì‹¤íŒ¨ (ë¶„ì„ì€ ì„±ê³µ): {str(e)}")
                        # ì €ì¥ ì‹¤íŒ¨í•´ë„ ë¶„ì„ëœ ë‰´ìŠ¤ëŠ” ë°˜í™˜
                        analyzed_news.append(news_data)

                else:
                    print(f"âš ï¸ AI ë¶„ì„ ì‹¤íŒ¨ (status {response.status_code}): {news['title'][:50]}...")

        except Exception as e:
            print(f"âŒ ë‰´ìŠ¤ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            continue

    print(f"âœ… AI ë¶„ì„ ì™„ë£Œ: {len(analyzed_news)}ê°œ ì„±ê³µ")
    return analyzed_news


async def get_news_hybrid(
    symbol: str,
    stock_name: Optional[str] = None,
    threshold_hours: int = 12,
    max_fresh_news: int = 10
) -> List[Dict]:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ë‰´ìŠ¤ ì¡°íšŒ (ë©”ì¸ í•¨ìˆ˜)

    ì „ëµ:
    1. DBì—ì„œ ìµœì‹  ë‰´ìŠ¤ ì¡°íšŒ
    2. ì‹ ì„ ë„ í™•ì¸ (< threshold_hours)
    3. ì˜¤ë˜ë˜ì—ˆìœ¼ë©´ ì‹¤ì‹œê°„ í¬ë¡¤ë§ + AI ë¶„ì„ + DB ì €ì¥
    4. DB ë‰´ìŠ¤ + ì‹ ê·œ ë‰´ìŠ¤ ë³‘í•© ë°˜í™˜

    Args:
        symbol: ì¢…ëª© ì½”ë“œ
        stock_name: ì¢…ëª©ëª… (ì„ íƒì‚¬í•­, ì—†ìœ¼ë©´ stock_masterì—ì„œ ì¡°íšŒ)
        threshold_hours: ì‹ ì„ ë„ ì„ê³„ê°’ (ê¸°ë³¸ 12ì‹œê°„)
        max_fresh_news: ì‹¤ì‹œê°„ í¬ë¡¤ë§ ìµœëŒ€ ê°œìˆ˜

    Returns:
        ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ (DB + ì‹ ê·œ ë³‘í•©)
    """
    try:
        # 0. ì¢…ëª©ëª…ì´ ì—†ìœ¼ë©´ stock_masterì—ì„œ ì¡°íšŒ
        if not stock_name:
            try:
                stock_result = get_supabase_client().table("stock_master") \
                    .select("name") \
                    .eq("symbol", symbol) \
                    .limit(1) \
                    .execute()

                if stock_result.data and len(stock_result.data) > 0:
                    stock_name = stock_result.data[0]["name"]
                    print(f"âœ… ì¢…ëª©ëª… ì¡°íšŒ: {symbol} â†’ {stock_name}")
                else:
                    stock_name = symbol  # í´ë°±: ì¢…ëª© ì½”ë“œ ì‚¬ìš©
                    print(f"âš ï¸ ì¢…ëª©ëª… ì¡°íšŒ ì‹¤íŒ¨, ì¢…ëª© ì½”ë“œ ì‚¬ìš©: {symbol}")

            except Exception as e:
                stock_name = symbol  # í´ë°±: ì¢…ëª© ì½”ë“œ ì‚¬ìš©
                print(f"âš ï¸ stock_master ì¡°íšŒ ì˜¤ë¥˜: {str(e)}, ì¢…ëª© ì½”ë“œ ì‚¬ìš©")

        # 1. ì‹ ì„ ë„ í™•ì¸
        is_fresh, latest_timestamp = await check_news_freshness(symbol, threshold_hours)

        # 2. ì‹¤ì‹œê°„ í¬ë¡¤ë§ í•„ìš” ì—¬ë¶€ íŒë‹¨
        fresh_news = []
        if not is_fresh:
            # ì‹¤ì‹œê°„ í¬ë¡¤ë§ íŠ¸ë¦¬ê±°
            raw_news = await fetch_realtime_news(symbol, stock_name, max_fresh_news)

            if raw_news:
                # AI ë¶„ì„ + DB ì €ì¥
                fresh_news = await analyze_and_save_news(raw_news, symbol)
            else:
                print("âš ï¸ ì‹¤ì‹œê°„ í¬ë¡¤ë§ ê²°ê³¼ ì—†ìŒ")

        # 3. DBì—ì„œ ë‰´ìŠ¤ ì¡°íšŒ (ìµœê·¼ 7ì¼, ìµœëŒ€ 50ê°œ)
        seven_days_ago = (datetime.now(timezone.utc) - timedelta(days=7)).isoformat()

        db_news_result = get_supabase_client().table("news") \
            .select("id, title, summary, sentiment_score, impact_score, published_at, url") \
            .contains("related_symbols", [symbol]) \
            .gte("published_at", seven_days_ago) \
            .order("impact_score", desc=True) \
            .order("published_at", desc=True) \
            .limit(50) \
            .execute()

        db_news = db_news_result.data or []

        # 4. ë³‘í•© (ì¤‘ë³µ ì œê±° - URL ê¸°ì¤€)
        seen_urls = set()
        combined_news = []

        # ì‹ ê·œ ë‰´ìŠ¤ ìš°ì„  ì¶”ê°€
        for news in fresh_news:
            url = news.get("url")
            if url and url not in seen_urls:
                seen_urls.add(url)
                combined_news.append(news)

        # DB ë‰´ìŠ¤ ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
        for news in db_news:
            url = news.get("url")
            if url and url not in seen_urls:
                seen_urls.add(url)
                combined_news.append(news)

        print(f"ğŸ“Š ìµœì¢… ë°˜í™˜: DB {len(db_news)}ê°œ + ì‹ ê·œ {len(fresh_news)}ê°œ = ì´ {len(combined_news)}ê°œ")
        return combined_news

    except Exception as e:
        print(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ë‰´ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        # í´ë°±: DB ì „ìš© ì¡°íšŒ
        return await get_news_db_only(symbol)


async def get_news_db_only(symbol: str) -> List[Dict]:
    """
    DB ì „ìš© ë‰´ìŠ¤ ì¡°íšŒ (í´ë°±ìš©)

    Args:
        symbol: ì¢…ëª© ì½”ë“œ

    Returns:
        DB ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    try:
        seven_days_ago = (datetime.now(timezone.utc) - timedelta(days=7)).isoformat()

        result = get_supabase_client().table("news") \
            .select("id, title, summary, sentiment_score, impact_score, published_at, url") \
            .contains("related_symbols", [symbol]) \
            .gte("published_at", seven_days_ago) \
            .order("impact_score", desc=True) \
            .order("published_at", desc=True) \
            .limit(50) \
            .execute()

        return result.data or []

    except Exception as e:
        print(f"âŒ DB ì „ìš© ë‰´ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return []
