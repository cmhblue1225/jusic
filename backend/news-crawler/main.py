"""
ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ë©”ì¸ ì„œë¹„ìŠ¤
ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ APIë¥¼ í†µí•´ ì¢…ëª©ë³„ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  Supabaseì— ì €ì¥
"""
import os
from fastapi import FastAPI
from apscheduler.schedulers.background import BackgroundScheduler
from dotenv import load_dotenv
import httpx
from datetime import datetime
from supabase import create_client, Client
from nlp.ner import StockNER
from naver_api import NaverNewsAPI

load_dotenv()

app = FastAPI(title="News Crawler Service")

# Supabase í´ë¼ì´ì–¸íŠ¸
supabase: Client = create_client(
    os.getenv("SUPABASE_URL", ""),
    os.getenv("SUPABASE_SERVICE_KEY", "")
)

# AI Service URL
AI_SERVICE_URL = os.getenv("AI_SERVICE_URL", "http://localhost:3003")

# ì¢…ëª©ëª… ì¶”ì¶œê¸° ì´ˆê¸°í™”
stock_ner = StockNER(supabase)

# ë„¤ì´ë²„ API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
naver_api = NaverNewsAPI()


async def analyze_news_with_ai(title: str, content: str, symbols: list, url: str) -> dict:
    """AI ì„œë¹„ìŠ¤ë¡œ ë‰´ìŠ¤ ë¶„ì„ (URL í¬í•¨ìœ¼ë¡œ ìºì‹± í™œìš©)"""
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                f"{AI_SERVICE_URL}/analyze",
                json={
                    "title": title,
                    "content": content,
                    "symbols": symbols,
                    "url": url  # ìºì‹±ì„ ìœ„í•œ URL ì¶”ê°€
                }
            )

            if response.status_code == 200:
                return response.json()
            else:
                print(f"âš ï¸ AI ë¶„ì„ ì‹¤íŒ¨ (status {response.status_code})")
                return None

    except Exception as e:
        print(f"âŒ AI ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        return None


async def get_top_stocks(limit: int = 50) -> list:
    """ì‹œê°€ì´ì•¡ ê¸°ì¤€ ìƒìœ„ ì¢…ëª© ì¡°íšŒ"""
    try:
        result = supabase.table("stock_master") \
            .select("symbol, symbol_name, market_cap") \
            .order("market_cap", desc=True) \
            .limit(limit) \
            .execute()

        if result.data:
            print(f"ğŸ“Š ìƒìœ„ {len(result.data)}ê°œ ì¢…ëª© ì¡°íšŒ ì™„ë£Œ")
            return [{"symbol": item["symbol"], "name": item["symbol_name"]} for item in result.data]
        else:
            print("âš ï¸ stock_master í…Œì´ë¸”ì—ì„œ ì¢…ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì¢…ëª© ì‚¬ìš©")
            return [
                {"symbol": "005930", "name": "ì‚¼ì„±ì „ì"},
                {"symbol": "000660", "name": "SKí•˜ì´ë‹‰ìŠ¤"},
                {"symbol": "035420", "name": "NAVER"},
                {"symbol": "035720", "name": "ì¹´ì¹´ì˜¤"},
                {"symbol": "051910", "name": "LGí™”í•™"},
            ]
    except Exception as e:
        print(f"âŒ ì¢…ëª© ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        # í´ë°±: ê¸°ë³¸ ì¢…ëª© ë°˜í™˜
        return [
            {"symbol": "005930", "name": "ì‚¼ì„±ì „ì"},
            {"symbol": "000660", "name": "SKí•˜ì´ë‹‰ìŠ¤"},
            {"symbol": "035420", "name": "NAVER"},
            {"symbol": "035720", "name": "ì¹´ì¹´ì˜¤"},
            {"symbol": "051910", "name": "LGí™”í•™"},
        ]


async def crawl_news():
    """ë„¤ì´ë²„ APIë¥¼ ì‚¬ìš©í•œ ë‰´ìŠ¤ í¬ë¡¤ë§"""
    print(f"[{datetime.now()}] ë„¤ì´ë²„ API ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘...")

    # 1. ìƒìœ„ ì¢…ëª© ì¡°íšŒ
    top_stocks = await get_top_stocks(limit=50)
    stock_names = [stock["name"] for stock in top_stocks]

    print(f"ğŸ¯ íƒ€ê²Ÿ ì¢…ëª©: {len(stock_names)}ê°œ")

    # 2. ë„¤ì´ë²„ APIë¡œ ì¢…ëª©ë³„ ë‰´ìŠ¤ ê²€ìƒ‰ (ì¢…ëª©ë‹¹ 5ê°œ)
    try:
        all_news = await naver_api.search_multiple_stocks(
            stock_names=stock_names,
            results_per_stock=5
        )

        print(f"ğŸ“° ì´ {len(all_news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ (ì¤‘ë³µ ì œê±° í›„)")

        # API ì‚¬ìš©ëŸ‰ ë¡œê¹…
        api_calls = len(stock_names) * 5
        print(f"ğŸ“Š API í˜¸ì¶œ ìˆ˜: {api_calls}ê°œ (ì¼ì¼ í•œë„: 25,000)")

    except Exception as e:
        print(f"âŒ ë„¤ì´ë²„ API í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}")
        return

    # 3. ê° ë‰´ìŠ¤ ì²˜ë¦¬
    new_count = 0
    duplicate_count = 0

    for news_item in all_news:
        try:
            # ì¤‘ë³µ ì²´í¬ (URL ê¸°ì¤€)
            existing = supabase.table("news").select("id").eq("url", news_item["url"]).execute()

            if existing.data:
                duplicate_count += 1
                continue

            title = news_item["title"]
            content = news_item["content"]
            url = news_item["url"]

            # 4. NERë¡œ ì¢…ëª© ì½”ë“œ ì¶”ì¶œ ë° ê²€ì¦
            full_text = f"{title} {content}"
            related_symbols = stock_ner.extract_symbols(full_text)

            print(f"\nğŸ“° ìƒˆ ë‰´ìŠ¤: {title[:50]}...")
            print(f"   URL: {url}")
            print(f"   NER ì¶”ì¶œ ì¢…ëª©: {related_symbols}")

            # 5. AI ë¶„ì„ ìš”ì²­ (URL í¬í•¨ìœ¼ë¡œ ìºì‹± í™œìš©)
            ai_result = await analyze_news_with_ai(title, content, related_symbols, url)

            # 6. Supabaseì— ì €ì¥
            news_data = {
                "source": news_item["source"],
                "title": title,
                "content": content,
                "url": url,
                "published_at": news_item["published_at"],
                "related_symbols": related_symbols,
            }

            # AI ë¶„ì„ ê²°ê³¼ ì¶”ê°€
            if ai_result:
                news_data.update({
                    "summary": ai_result.get("summary"),
                    "sentiment_score": ai_result.get("sentiment_score"),
                    "impact_score": ai_result.get("impact_score"),
                    "recommended_action": ai_result.get("recommended_action"),
                })
                print(f"   AI ë¶„ì„ ì™„ë£Œ - ì˜í–¥ë„: {ai_result.get('impact_score'):.2f}, ê°ì •: {ai_result.get('sentiment_score'):.2f}, ê¶Œê³ : {ai_result.get('recommended_action')}")

            # DB ì €ì¥
            result = supabase.table("news").insert(news_data).execute()
            new_count += 1
            print(f"âœ… ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ")

        except Exception as e:
            print(f"âŒ ë‰´ìŠ¤ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            continue

    print(f"\n[{datetime.now()}] ë„¤ì´ë²„ API ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ")
    print(f"ğŸ“ˆ í†µê³„: ì‹ ê·œ {new_count}ê°œ, ì¤‘ë³µ {duplicate_count}ê°œ\n")


@app.on_event("startup")
async def startup_event():
    """ì•± ì‹œì‘ ì‹œ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰"""
    scheduler = BackgroundScheduler()
    # 5ë¶„ë§ˆë‹¤ ë‰´ìŠ¤ í¬ë¡¤ë§
    scheduler.add_job(crawl_news, 'interval', minutes=5)
    scheduler.start()
    print("ğŸ“° News Crawler Scheduler started (every 5 minutes)")


@app.get("/health")
async def health():
    return {"status": "ok", "service": "news-crawler"}


@app.post("/crawl")
async def trigger_crawl():
    """ìˆ˜ë™ í¬ë¡¤ë§ íŠ¸ë¦¬ê±°"""
    await crawl_news()
    return {"message": "Crawling triggered"}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=3002)
