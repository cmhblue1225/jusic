"""
ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ë©”ì¸ ì„œë¹„ìŠ¤
ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API + Google News RSSë¥¼ í†µí•´ ì¢…ëª©ë³„ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  Supabaseì— ì €ì¥

ğŸ”¥ Phase 2.1: Google News RSS ì¶”ê°€
"""
import os
from fastapi import FastAPI, HTTPException
from apscheduler.schedulers.background import BackgroundScheduler
import asyncio
from dotenv import load_dotenv
import httpx
from datetime import datetime, timedelta, timezone
from supabase import create_client, Client
from nlp.ner import StockNER
from naver_api import NaverNewsAPI
from google_news_rss import GoogleNewsRSS  # ğŸ”¥ Phase 2.1
from naver_discussion_crawler import NaverDiscussionCrawler  # ğŸ”¥ Phase 2.2
from dart_disclosure_crawler import DartDisclosureCrawler  # ğŸ”¥ Phase 2.3

load_dotenv()

app = FastAPI(title="News Crawler Service")

# Supabase í´ë¼ì´ì–¸íŠ¸
supabase: Client = create_client(
    os.getenv("SUPABASE_URL", ""),
    os.getenv("SUPABASE_SERVICE_KEY", "")
)

# AI Service URL
AI_SERVICE_URL = os.getenv("AI_SERVICE_URL", "http://localhost:3003")

# ğŸ”¥ AI ë¶„ì„ í™œì„±í™” ì—¬ë¶€ (í™˜ê²½ ë³€ìˆ˜ë¡œ ì œì–´)
AI_ANALYSIS_ENABLED = os.getenv("AI_ANALYSIS_ENABLED", "true").lower() == "true"

# ì¢…ëª©ëª… ì¶”ì¶œê¸° ì´ˆê¸°í™”
stock_ner = StockNER(supabase)

# ë„¤ì´ë²„ API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
naver_api = NaverNewsAPI()

# ğŸ”¥ Phase 2.1: Google News RSS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
google_news = GoogleNewsRSS()

# ğŸ”¥ Phase 2.2: ë„¤ì´ë²„ í† ë¡ ë°© í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
naver_discussion = NaverDiscussionCrawler()

# ğŸ”¥ Phase 2.3: DART ì „ìê³µì‹œ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
dart_crawler = DartDisclosureCrawler()

# ğŸ”¥ ìŠ¤ì¼€ì¤„ëŸ¬ ì „ì—­ ë³€ìˆ˜ (ê´€ë¦¬ì ì œì–´ìš©)
scheduler: BackgroundScheduler = None


async def analyze_news_with_ai(title: str, content: str, symbols: list, url: str) -> dict:
    """AI ì„œë¹„ìŠ¤ë¡œ ë‰´ìŠ¤ ë¶„ì„ (URL í¬í•¨ìœ¼ë¡œ ìºì‹± í™œìš©)"""
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                f"{AI_SERVICE_URL}/analyze",
                json={
                    "title": title,
                    "content": content,
                    "symbols": symbols,
                    "url": url  # ìºì‹±ì„ ìœ„í•œ URL ì¶”ê°€
                }
            )

            if response.status_code == 200:
                return response.json()
            else:
                print(f"âš ï¸ AI ë¶„ì„ ì‹¤íŒ¨ (status {response.status_code})")
                return None

    except Exception as e:
        print(f"âŒ AI ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
        return None


async def create_alerts_for_news(news_data: dict, ai_result: dict):
    """
    ì˜í–¥ë„ê°€ ë†’ì€ ë‰´ìŠ¤ì— ëŒ€í•´ ê´€ë ¨ ì¢…ëª©ì„ ë³´ìœ /ê´€ì‹¬ ì¤‘ì¸ ì‚¬ìš©ìì—ê²Œ ì•Œë¦¼ ìƒì„±

    Args:
        news_data: ë‰´ìŠ¤ ë°ì´í„°
        ai_result: AI ë¶„ì„ ê²°ê³¼
    """
    try:
        related_symbols = news_data.get("related_symbols", [])
        if not related_symbols:
            return

        impact_score = ai_result.get("impact_score", 0)
        sentiment_score = ai_result.get("sentiment_score", 0)
        recommended_action = ai_result.get("recommended_action", "hold")

        # 1. í•´ë‹¹ ì¢…ëª©ì„ ë³´ìœ /ê´€ì‹¬ ì¤‘ì¸ ì‚¬ìš©ì ì¡°íšŒ
        portfolio_users = supabase.table("portfolios") \
            .select("user_id") \
            .in_("symbol", related_symbols) \
            .execute()

        watchlist_users = supabase.table("watchlist") \
            .select("user_id") \
            .in_("symbol", related_symbols) \
            .execute()

        # ì‚¬ìš©ì ID ì¤‘ë³µ ì œê±°
        user_ids = set()
        if portfolio_users.data:
            user_ids.update([item["user_id"] for item in portfolio_users.data])
        if watchlist_users.data:
            user_ids.update([item["user_id"] for item in watchlist_users.data])

        if not user_ids:
            print(f"   â„¹ï¸ ê´€ë ¨ ì‚¬ìš©ì ì—†ìŒ (ì¢…ëª©: {related_symbols})")
            return

        # 2. ê° ì‚¬ìš©ìì—ê²Œ ì•Œë¦¼ ìƒì„±
        alerts = []
        for user_id in user_ids:
            # ê°ì„±ì— ë”°ë¥¸ ì´ëª¨ì§€
            emoji = "ğŸ“ˆ" if sentiment_score > 0 else "ğŸ“‰" if sentiment_score < 0 else "ğŸ“Š"

            # ê¶Œê³ ì— ë”°ë¥¸ ì•¡ì…˜ í…ìŠ¤íŠ¸
            action_text = {
                "buy": "ë§¤ìˆ˜ ê²€í† ",
                "sell": "ë§¤ë„ ê²€í† ",
                "hold": "ê´€ë§ ê¶Œì¥"
            }.get(recommended_action, "ì •ë³´ í™•ì¸")

            alert = {
                "user_id": user_id,
                "type": "news",
                "title": f"{emoji} ì¤‘ìš” ë‰´ìŠ¤ ({', '.join(related_symbols[:3])})",
                "message": f"{news_data['title'][:100]}... [{action_text}]",
                "params": {
                    "news_url": news_data.get("url"),
                    "impact_score": impact_score,
                    "sentiment_score": sentiment_score,
                    "recommended_action": recommended_action,
                    "related_symbols": related_symbols,
                },
                "status": "unread",
            }
            alerts.append(alert)

        # 3. ë°°ì¹˜ ì‚½ì…
        if alerts:
            supabase.table("alerts").insert(alerts).execute()
            print(f"   ğŸ”” ì•Œë¦¼ ìƒì„± ì™„ë£Œ: {len(alerts)}ëª… ì‚¬ìš©ì")

    except Exception as e:
        print(f"âŒ ì•Œë¦¼ ìƒì„± ì˜¤ë¥˜: {str(e)}")


async def get_user_tracked_stocks() -> list:
    """ëª¨ë“  ì‚¬ìš©ìì˜ ë³´ìœ  ì¢…ëª© + ê´€ì‹¬ ì¢…ëª© ì¡°íšŒ (ì¤‘ë³µ ì œê±°)"""
    try:
        # 1. ë³´ìœ  ì¢…ëª© ì¡°íšŒ
        portfolio_result = supabase.table("portfolios") \
            .select("symbol") \
            .execute()

        # 2. ê´€ì‹¬ ì¢…ëª© ì¡°íšŒ
        watchlist_result = supabase.table("watchlist") \
            .select("symbol") \
            .execute()

        # 3. ì¢…ëª© ì½”ë“œ í•©ì¹˜ê³  ì¤‘ë³µ ì œê±°
        portfolio_symbols = {item["symbol"] for item in (portfolio_result.data or [])}
        watchlist_symbols = {item["symbol"] for item in (watchlist_result.data or [])}
        all_symbols = portfolio_symbols | watchlist_symbols  # ì§‘í•© í•©ì§‘í•© (ì¤‘ë³µ ìë™ ì œê±°)

        if not all_symbols:
            print("âš ï¸ ì‚¬ìš©ìì˜ ë³´ìœ /ê´€ì‹¬ ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì¢…ëª© ì‚¬ìš©")
            return [
                {"symbol": "005930", "name": "ì‚¼ì„±ì „ì"},
                {"symbol": "000660", "name": "SKí•˜ì´ë‹‰ìŠ¤"},
                {"symbol": "035420", "name": "NAVER"},
                {"symbol": "035720", "name": "ì¹´ì¹´ì˜¤"},
                {"symbol": "051910", "name": "LGí™”í•™"},
            ]

        # 4. stock_masterì—ì„œ ì¢…ëª©ëª… ì¡°íšŒ
        symbols_list = list(all_symbols)
        stock_master_result = supabase.table("stock_master") \
            .select("symbol, name") \
            .in_("symbol", symbols_list) \
            .execute()

        if stock_master_result.data:
            print(f"ğŸ“Š ì‚¬ìš©ì ì¶”ì  ì¢…ëª© {len(stock_master_result.data)}ê°œ ì¡°íšŒ ì™„ë£Œ (ë³´ìœ  {len(portfolio_symbols)}ê°œ + ê´€ì‹¬ {len(watchlist_symbols)}ê°œ)")
            return [{"symbol": item["symbol"], "name": item["name"]} for item in stock_master_result.data]
        else:
            print("âš ï¸ stock_masterì—ì„œ ì¢…ëª© ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì¢…ëª© ì‚¬ìš©")
            return [
                {"symbol": "005930", "name": "ì‚¼ì„±ì „ì"},
                {"symbol": "000660", "name": "SKí•˜ì´ë‹‰ìŠ¤"},
                {"symbol": "035420", "name": "NAVER"},
                {"symbol": "035720", "name": "ì¹´ì¹´ì˜¤"},
                {"symbol": "051910", "name": "LGí™”í•™"},
            ]
    except Exception as e:
        print(f"âŒ ì¢…ëª© ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        # í´ë°±: ê¸°ë³¸ ì¢…ëª© ë°˜í™˜
        return [
            {"symbol": "005930", "name": "ì‚¼ì„±ì „ì"},
            {"symbol": "000660", "name": "SKí•˜ì´ë‹‰ìŠ¤"},
            {"symbol": "035420", "name": "NAVER"},
            {"symbol": "035720", "name": "ì¹´ì¹´ì˜¤"},
            {"symbol": "051910", "name": "LGí™”í•™"},
        ]


async def crawl_news():
    """ğŸ”¥ Phase 2.1: ë„¤ì´ë²„ API + Google News RSS ë‰´ìŠ¤ í¬ë¡¤ë§"""
    print(f"[{datetime.now()}] ë©€í‹° ì†ŒìŠ¤ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘...")

    # 1. ì‚¬ìš©ì ì¶”ì  ì¢…ëª© ì¡°íšŒ (ë³´ìœ  + ê´€ì‹¬)
    tracked_stocks = await get_user_tracked_stocks()
    stock_names = [stock["name"] for stock in tracked_stocks]

    print(f"ğŸ¯ ì‚¬ìš©ì ì¶”ì  ì¢…ëª©: {len(stock_names)}ê°œ")

    # 2. ë„¤ì´ë²„ APIë¡œ ì¢…ëª©ë³„ ë‰´ìŠ¤ ê²€ìƒ‰ (ì¢…ëª©ë‹¹ 10ê°œ)
    naver_news = []
    try:
        naver_news = await naver_api.search_multiple_stocks(
            stock_names=stock_names,
            results_per_stock=10
        )

        print(f"ğŸ“° [Naver] {len(naver_news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ (ì¤‘ë³µ ì œê±° í›„)")

        # API ì‚¬ìš©ëŸ‰ ë¡œê¹…
        api_calls = len(stock_names) * 10
        print(f"ğŸ“Š [Naver] API í˜¸ì¶œ ìˆ˜: {api_calls}ê°œ (ì¼ì¼ í•œë„: 25,000)")

    except Exception as e:
        print(f"âš ï¸ [Naver] API í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}")

    # ğŸ”¥ Phase 2.1: Google News RSSë¡œ ì¶”ê°€ ë‰´ìŠ¤ ê²€ìƒ‰ (ì¢…ëª©ë‹¹ 5ê°œ)
    google_news_list = []
    try:
        google_news_list = await google_news.search_multiple_stocks(
            stock_names=stock_names,
            results_per_stock=5
        )

        print(f"ğŸ“° [Google News] {len(google_news_list)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ (ì¤‘ë³µ ì œê±° í›„)")

    except Exception as e:
        print(f"âš ï¸ [Google News] RSS í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")

    # 3. ë‘ ì†ŒìŠ¤ ë³‘í•© ë° URL ê¸°ì¤€ ì¤‘ë³µ ì œê±°
    all_news = naver_news + google_news_list
    seen_urls = set()
    unique_news = []

    for news in all_news:
        if news["url"] not in seen_urls:
            seen_urls.add(news["url"])
            unique_news.append(news)

    all_news = unique_news
    print(f"ğŸ“Š [í†µí•©] ì´ {len(all_news)}ê°œ ë‰´ìŠ¤ (Naver + Google News, ì¤‘ë³µ ì œê±° í›„)")

    if not all_news:
        print("âš ï¸ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 3. ê° ë‰´ìŠ¤ ì²˜ë¦¬
    new_count = 0
    duplicate_count = 0
    old_news_count = 0

    # 3ì¼ ì´ì „ ì‹œê°„ ê³„ì‚° (ìµœì‹  ë‰´ìŠ¤ ìœ„ì£¼)
    # UTC timezone aware datetime ì‚¬ìš©
    cutoff_time = datetime.now(timezone.utc) - timedelta(days=3)

    for news_item in all_news:
        try:
            # ë°œí–‰ ì‹œê°„ ì²´í¬ (3ì¼ ì´ë‚´ë§Œ ì²˜ë¦¬)
            published_at = datetime.fromisoformat(news_item["published_at"].replace('Z', '+00:00'))
            if published_at < cutoff_time:
                old_news_count += 1
                continue

            # ì¤‘ë³µ ì²´í¬ (URL ê¸°ì¤€)
            existing = supabase.table("news").select("id").eq("url", news_item["url"]).execute()

            if existing.data:
                duplicate_count += 1
                continue

            title = news_item["title"]
            content = news_item["content"]
            url = news_item["url"]

            # 4. NERë¡œ ì¢…ëª© ì½”ë“œ ì¶”ì¶œ ë° ê²€ì¦
            full_text = f"{title} {content}"
            related_symbols = stock_ner.extract_symbols(full_text)

            print(f"\nğŸ“° ìƒˆ ë‰´ìŠ¤: {title[:50]}...")
            print(f"   URL: {url}")
            print(f"   NER ì¶”ì¶œ ì¢…ëª©: {related_symbols}")

            # 5. AI ë¶„ì„ ìš”ì²­ (í™˜ê²½ ë³€ìˆ˜ë¡œ ì œì–´)
            ai_result = None
            if AI_ANALYSIS_ENABLED:
                ai_result = await analyze_news_with_ai(title, content, related_symbols, url)
            else:
                print(f"   â¸ï¸ AI ë¶„ì„ ë¹„í™œì„±í™”ë¨ (AI_ANALYSIS_ENABLED=false)")

            # 6. Supabaseì— ì €ì¥
            news_data = {
                "source": news_item["source"],
                "title": title,
                "content": content,
                "url": url,
                "published_at": news_item["published_at"],
                "related_symbols": related_symbols,
            }

            # AI ë¶„ì„ ê²°ê³¼ ì¶”ê°€
            if ai_result:
                news_data.update({
                    "summary": ai_result.get("summary"),
                    "sentiment_score": ai_result.get("sentiment_score"),
                    "impact_score": ai_result.get("impact_score"),
                    "recommended_action": ai_result.get("recommended_action"),
                })
                print(f"   AI ë¶„ì„ ì™„ë£Œ - ì˜í–¥ë„: {ai_result.get('impact_score'):.2f}, ê°ì •: {ai_result.get('sentiment_score'):.2f}, ê¶Œê³ : {ai_result.get('recommended_action')}")

            # DB ì €ì¥
            result = supabase.table("news").insert(news_data).execute()
            new_count += 1
            print(f"âœ… ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ")

            # 7. ì˜í–¥ë„ ê¸°ë°˜ ì•Œë¦¼ ìƒì„± (impact_score >= 0.7)
            if ai_result and ai_result.get("impact_score", 0) >= 0.7:
                await create_alerts_for_news(news_data, ai_result)

        except Exception as e:
            print(f"âŒ ë‰´ìŠ¤ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            continue

    print(f"\n[{datetime.now()}] ë©€í‹° ì†ŒìŠ¤ ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ (Naver + Google News)")
    print(f"ğŸ“ˆ í†µê³„: ì‹ ê·œ {new_count}ê°œ, ì¤‘ë³µ {duplicate_count}ê°œ, 3ì¼ ì´ì „ {old_news_count}ê°œ\n")


def crawl_news_sync():
    """ë™ê¸° wrapper í•¨ìˆ˜ - ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ í˜¸ì¶œ"""
    # ìƒˆ ì´ë²¤íŠ¸ ë£¨í”„ ìƒì„± ë° ì‹¤í–‰
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        loop.run_until_complete(crawl_news())
    finally:
        loop.close()


@app.on_event("startup")
async def startup_event():
    """ì•± ì‹œì‘ ì‹œ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰"""
    global scheduler
    scheduler = BackgroundScheduler()
    # 5ë¶„ë§ˆë‹¤ ë‰´ìŠ¤ í¬ë¡¤ë§
    scheduler.add_job(crawl_news_sync, 'interval', minutes=5)
    scheduler.start()
    print("ğŸ“° News Crawler Scheduler started (every 5 minutes)")
    print(f"ğŸ¤– AI ë¶„ì„: {'âœ… í™œì„±í™”' if AI_ANALYSIS_ENABLED else 'â¸ï¸ ë¹„í™œì„±í™”'}")

    # ì‹œì‘ ì‹œ ì¦‰ì‹œ í•œ ë²ˆ ì‹¤í–‰
    print("ğŸš€ ì´ˆê¸° í¬ë¡¤ë§ ì‹¤í–‰...")
    asyncio.create_task(crawl_news())


@app.get("/health")
async def health():
    return {"status": "ok", "service": "news-crawler"}


@app.post("/crawl")
async def trigger_crawl():
    """ìˆ˜ë™ í¬ë¡¤ë§ íŠ¸ë¦¬ê±°"""
    await crawl_news()
    return {"message": "Crawling triggered"}


# ğŸ”¥ ê´€ë¦¬ì ì œì–´ ì—”ë“œí¬ì¸íŠ¸
@app.post("/admin/scheduler/pause")
async def pause_scheduler():
    """ìŠ¤ì¼€ì¤„ëŸ¬ ì¼ì‹œì¤‘ì§€ (ê´€ë¦¬ì ì „ìš©)"""
    global scheduler
    if scheduler is None:
        raise HTTPException(status_code=500, detail="Scheduler not initialized")

    try:
        scheduler.pause()
        return {
            "status": "paused",
            "message": "News crawler scheduler has been paused"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to pause scheduler: {str(e)}")


@app.post("/admin/scheduler/resume")
async def resume_scheduler():
    """ìŠ¤ì¼€ì¤„ëŸ¬ ì¬ê°œ (ê´€ë¦¬ì ì „ìš©)"""
    global scheduler
    if scheduler is None:
        raise HTTPException(status_code=500, detail="Scheduler not initialized")

    try:
        scheduler.resume()
        return {
            "status": "running",
            "message": "News crawler scheduler has been resumed"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to resume scheduler: {str(e)}")


@app.get("/admin/scheduler/status")
async def get_scheduler_status():
    """ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ì¡°íšŒ"""
    global scheduler
    if scheduler is None:
        return {
            "status": "not_initialized",
            "state": None,
            "jobs": []
        }

    # APScheduler ìƒíƒœ: 0=stopped, 1=running, 2=paused
    state_map = {0: "stopped", 1: "running", 2: "paused"}
    state_code = scheduler.state
    status = state_map.get(state_code, "unknown")

    # ë“±ë¡ëœ job ëª©ë¡
    jobs = []
    for job in scheduler.get_jobs():
        jobs.append({
            "id": job.id,
            "name": job.name,
            "next_run_time": job.next_run_time.isoformat() if job.next_run_time else None
        })

    return {
        "status": status,
        "state": state_code,
        "jobs": jobs,
        "ai_analysis_enabled": AI_ANALYSIS_ENABLED
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=3002)
