"""
ğŸ”¥ Phase 2.1: Google News RSS í¬ë¡¤ëŸ¬
- Google News RSS Feedë¥¼ í†µí•´ ì¢…ëª© ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘
- ë„¤ì´ë²„ API ë³´ì™„ìš© ì¶”ê°€ ë‰´ìŠ¤ ì†ŒìŠ¤
"""
import feedparser
import httpx
from datetime import datetime, timezone
from typing import List, Dict, Any
from urllib.parse import quote_plus


class GoogleNewsRSS:
    """Google News RSS Feed í¬ë¡¤ëŸ¬"""

    def __init__(self):
        self.base_url = "https://news.google.com/rss/search"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
        }

    async def search_stock_news(
        self,
        stock_name: str,
        max_results: int = 10
    ) -> List[Dict[str, Any]]:
        """
        íŠ¹ì • ì¢…ëª©ëª…ìœ¼ë¡œ Google News ê²€ìƒ‰

        Args:
            stock_name: ì¢…ëª©ëª… (ì˜ˆ: "ì‚¼ì„±ì „ì")
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜

        Returns:
            List[Dict]: ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
                - title: ì œëª©
                - url: URL
                - published_at: ë°œí–‰ ì‹œê°„ (ISO 8601)
                - source: ì¶œì²˜ (ì˜ˆ: "Google News")
                - content: ìš”ì•½ (RSS description)
        """
        try:
            # Google News RSS ê²€ìƒ‰ ì¿¼ë¦¬
            # ì˜ˆ: https://news.google.com/rss/search?q=ì‚¼ì„±ì „ì+ì£¼ì‹&hl=ko&gl=KR&ceid=KR:ko
            query = f"{stock_name} ì£¼ì‹"
            encoded_query = quote_plus(query)

            rss_url = (
                f"{self.base_url}?q={encoded_query}"
                f"&hl=ko"  # í•œêµ­ì–´
                f"&gl=KR"  # ëŒ€í•œë¯¼êµ­
                f"&ceid=KR:ko"  # í•œêµ­ ë‰´ìŠ¤
            )

            print(f"ğŸ“° [Google News RSS] ê²€ìƒ‰: {stock_name} (URL: {rss_url[:80]}...)")

            # HTTP ìš”ì²­ (ë¹„ë™ê¸°)
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(rss_url, headers=self.headers)
                response.raise_for_status()

            # RSS íŒŒì‹±
            feed = feedparser.parse(response.text)

            if not feed.entries:
                print(f"âš ï¸ [Google News RSS] ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: {stock_name}")
                return []

            # ê²°ê³¼ ë³€í™˜
            news_list = []
            for entry in feed.entries[:max_results]:
                try:
                    # ë°œí–‰ ì‹œê°„ íŒŒì‹± (feedparserëŠ” struct_time ë°˜í™˜)
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        published_dt = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                        published_iso = published_dt.isoformat()
                    else:
                        published_iso = datetime.now(timezone.utc).isoformat()

                    # ì½˜í…ì¸  ì¶”ì¶œ (description ë˜ëŠ” summary)
                    content = ""
                    if hasattr(entry, 'summary'):
                        content = entry.summary
                    elif hasattr(entry, 'description'):
                        content = entry.description

                    # HTML íƒœê·¸ ì œê±° (ê°„ë‹¨í•œ ì²˜ë¦¬)
                    if content:
                        import re
                        content = re.sub(r'<[^>]+>', '', content)
                        content = content.strip()

                    news_item = {
                        "title": entry.title,
                        "url": entry.link,
                        "published_at": published_iso,
                        "source": "Google News",
                        "content": content[:500] if content else ""  # 500ìë¡œ ì œí•œ
                    }

                    news_list.append(news_item)

                except Exception as e:
                    print(f"âš ï¸ [Google News RSS] í•­ëª© íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                    continue

            print(f"âœ… [Google News RSS] {stock_name}: {len(news_list)}ê°œ ìˆ˜ì§‘")
            return news_list

        except httpx.HTTPError as e:
            print(f"âŒ [Google News RSS] HTTP ì˜¤ë¥˜: {str(e)}")
            return []
        except Exception as e:
            print(f"âŒ [Google News RSS] ì˜¤ë¥˜: {str(e)}")
            return []

    async def search_multiple_stocks(
        self,
        stock_names: List[str],
        results_per_stock: int = 5
    ) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ ì¢…ëª©ì— ëŒ€í•´ ë‰´ìŠ¤ ê²€ìƒ‰ (ì¤‘ë³µ ì œê±°)

        Args:
            stock_names: ì¢…ëª©ëª… ë¦¬ìŠ¤íŠ¸
            results_per_stock: ì¢…ëª©ë‹¹ ê²°ê³¼ ìˆ˜

        Returns:
            List[Dict]: ì¤‘ë³µ ì œê±°ëœ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        all_news = []
        seen_urls = set()

        for stock_name in stock_names:
            news_list = await self.search_stock_news(stock_name, results_per_stock)

            # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
            for news in news_list:
                if news["url"] not in seen_urls:
                    seen_urls.add(news["url"])
                    all_news.append(news)

        print(f"ğŸ“Š [Google News RSS] ì´ {len(all_news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ (ì¤‘ë³µ ì œê±° í›„)")
        return all_news


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    import asyncio

    async def test():
        crawler = GoogleNewsRSS()

        # ë‹¨ì¼ ì¢…ëª© í…ŒìŠ¤íŠ¸
        print("\n=== ë‹¨ì¼ ì¢…ëª© í…ŒìŠ¤íŠ¸ ===")
        news = await crawler.search_stock_news("ì‚¼ì„±ì „ì", max_results=5)
        for i, item in enumerate(news, 1):
            print(f"\n{i}. {item['title']}")
            print(f"   URL: {item['url'][:80]}...")
            print(f"   ë°œí–‰: {item['published_at']}")
            print(f"   ë‚´ìš©: {item['content'][:100]}...")

        # ë‹¤ì¤‘ ì¢…ëª© í…ŒìŠ¤íŠ¸
        print("\n\n=== ë‹¤ì¤‘ ì¢…ëª© í…ŒìŠ¤íŠ¸ ===")
        stocks = ["ì‚¼ì„±ì „ì", "SKí•˜ì´ë‹‰ìŠ¤", "NAVER"]
        all_news = await crawler.search_multiple_stocks(stocks, results_per_stock=3)
        print(f"\nì´ {len(all_news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")

    asyncio.run(test())
